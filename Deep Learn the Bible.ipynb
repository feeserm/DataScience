{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re, nltk, spacy, gensim\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('punkt')\n",
    "from nltk.collocations import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#standard functions\n",
    "#function to clean text\n",
    "def CleanTextColumn(TextCol, removelist=[]):\n",
    "    \"\"\"takes a list of text, and a list of words to clean out\n",
    "    \n",
    "    Returns the text column as a list with emails, new line characters, the words listed, \n",
    "    special characters, and single quotes, making all characters lower case\"\"\"\n",
    "    \n",
    "    # Convert to list if it's a pandas series\n",
    "    if type(TextCol) == type(pd.Series()):\n",
    "        TextCol = raw_df.astype(str).values.tolist()\n",
    "    \n",
    "    # Remove Emails\n",
    "    TextCol = [re.sub('\\S*@\\S*\\s?', '', sent) for sent in TextCol]\n",
    "    \n",
    "    #Remove double spaces\n",
    "    TextCol = [re.sub('  ', ' ', sent) for sent in TextCol]\n",
    "    \n",
    "    # Remove new line characters\n",
    "    TextCol = [re.sub('\\s+', ' ', sent) for sent in TextCol]\n",
    "    \n",
    "    # Convert data to lowercase\n",
    "    TextCol = [sent.lower() for sent in TextCol]\n",
    "    \n",
    "    # Remove single quotes\n",
    "    TextCol = [re.sub(\"\\'\", \"\", sent) for sent in TextCol]\n",
    "    \n",
    "    for junk in removelist:\n",
    "        TextCol = [re.sub(junk, \"\", sent) for sent in TextCol]\n",
    "    \n",
    "    # Removing special characters\n",
    "    TextCol = [re.sub('[^A-Za-z ]+', '', sent) for sent in TextCol]\n",
    "    \n",
    "    return TextCol\n",
    "\n",
    "#ignored_words = nltk.corpus.stopwords.words('english')\n",
    "#finder.apply_word_filter(lambda w: len(w) < 3 or w.lower() in ignored_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath.head()\n",
    "NASB = pd.read_excel(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NASB.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data preparation for word tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-15T23:00:07.216999Z",
     "start_time": "2019-11-15T23:00:07.198372Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_WordTree_data_from_Series(series,word,n):\n",
    "    \"\"\"iterates over a pandas text series and creates a \n",
    "    WordTree dataset of n words before and after each instance of word\"\"\"\n",
    "    wordtreedata = [['Phrases']]\n",
    "\n",
    "    for index, value in series.items():\n",
    "        wordtreedata.extend(get_WordTree_data(value,word,n))\n",
    "        \n",
    "    return wordtreedata\n",
    "\n",
    "def get_WordTree_data(text,word,n):\n",
    "    \"\"\"takes a text, a target word, and the number of words around it desired (n) and returns data prepared for WordTree\"\"\"\n",
    "    \n",
    "    #convert text to list of sequential words\n",
    "    text = remove_non_text(text)\n",
    "    textlist = text.split()\n",
    "    \n",
    "    #iterate over each instance of the word in the data and append its surrounding n words to the data\n",
    "    wordtreedata=[]\n",
    "    word_instances = get_word_instances(textlist,word)\n",
    "    \n",
    "    for instance in word_instances:\n",
    "        wordtreedata.append(get_words_before_after(textlist, instance, n))\n",
    "    return wordtreedata\n",
    "\n",
    "def remove_non_text(text):\n",
    "    \"\"\"removes doubled white space and non-alphabetic characters\"\"\"\n",
    "    text = re.sub(r'([^\\s\\w]|_|[1-9])+', '', text) #remove non alphanumeric/whitespace\n",
    "    text = re.sub(\" +\", \" \", text) #remove white space\n",
    "    return text\n",
    "\n",
    "def get_word_instances(textlist,word):\n",
    "    \"\"\"returns word index of each \"\"\"\n",
    "    return [i for i, w in enumerate(textlist) if w == word]\n",
    "\n",
    "def get_words_before_after(textlist,index,n):\n",
    "    \"\"\"takes a list of words and retrieves the n words before and after the word at index\"\"\"\n",
    "    \n",
    "    #initialize starting and finishing indices\n",
    "    low = index-n\n",
    "    high = index+n+1\n",
    "    \n",
    "    #filter to the last word or first word if n extends beyond the length or start of the text\n",
    "    if index - n < 0:\n",
    "        low = 0\n",
    "    elif index+n+1 >= len(textlist):\n",
    "        high = len(textlist)-1\n",
    "    \n",
    "    return [\" \".join(textlist[low:high])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-15T21:57:05.266428Z",
     "start_time": "2019-11-15T21:57:05.199537Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['slow', 'load', 'poor', 'pics', 'see', 'google'],\n",
       " ['slow'],\n",
       " ['so', 'slow'],\n",
       " ['Too', 'slow', 'to', 'load'],\n",
       " ['Powerpoint', 'is', 'so', 'slow'],\n",
       " ['very', 'very', 'slow'],\n",
       " ['this', 'is', 'so', 'slow'],\n",
       " ['to', 'slow'],\n",
       " ['Its', 'way', 'too', 'slow', 'Please', 'fix', 'the', 'speed'],\n",
       " ['Not', 'enought', 'options', 'very', 'slow', 'to', 'load'],\n",
       " ['loading', 'slow'],\n",
       " ['Very', 'slow', 'connecting', 'to', 'server'],\n",
       " ['Too', 'slow', 'search', 'engine'],\n",
       " ['slow'],\n",
       " ['too', 'slow'],\n",
       " ['too', 'slow'],\n",
       " ['extremly', 'slow'],\n",
       " ['The',\n",
       "  'speed',\n",
       "  'is',\n",
       "  'excruciatingly',\n",
       "  'slow',\n",
       "  'even',\n",
       "  'though',\n",
       "  'the',\n",
       "  'internet',\n",
       "  'is'],\n",
       " ['very', 'slow', 'to', 'load', 'the', 'library'],\n",
       " ['do',\n",
       "  'it',\n",
       "  'everything',\n",
       "  'is',\n",
       "  'really',\n",
       "  'slow',\n",
       "  'I',\n",
       "  'really',\n",
       "  'dislike',\n",
       "  'this',\n",
       "  'because'],\n",
       " ['but',\n",
       "  'its',\n",
       "  'just',\n",
       "  'PowerPoint',\n",
       "  'thats',\n",
       "  'slow',\n",
       "  'Please',\n",
       "  'fix',\n",
       "  'this',\n",
       "  'error',\n",
       "  'It'],\n",
       " ['to', 'slow'],\n",
       " ['slide',\n",
       "  'and',\n",
       "  'it',\n",
       "  'is',\n",
       "  'extremely',\n",
       "  'slow',\n",
       "  'and',\n",
       "  'not',\n",
       "  'user',\n",
       "  'friendly',\n",
       "  'I'],\n",
       " ['in',\n",
       "  'ink',\n",
       "  'and',\n",
       "  'is',\n",
       "  'extremely',\n",
       "  'slow',\n",
       "  'for',\n",
       "  'a',\n",
       "  'long',\n",
       "  'time',\n",
       "  'I'],\n",
       " ['Powerpoint', 'is', 'slow', 'when', 'I', 'type'],\n",
       " ['being', 'slow', 'as', 'heck'],\n",
       " ['On', 'slow', 'airplane', 'Wifi', 'I', 'am', 'not'],\n",
       " ['be',\n",
       "  'because',\n",
       "  'Im',\n",
       "  'on',\n",
       "  'a',\n",
       "  'slow',\n",
       "  'wifi',\n",
       "  'connection',\n",
       "  'and',\n",
       "  'PPT',\n",
       "  'is'],\n",
       " ['has', 'to', 'ru', 'faster', 'its', 'slow', 'to'],\n",
       " ['back',\n",
       "  'Less',\n",
       "  'stable',\n",
       "  'losing',\n",
       "  'work',\n",
       "  'slow',\n",
       "  'can',\n",
       "  'it',\n",
       "  'still',\n",
       "  'get']]"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Thu Nov 14 10:56:54 2019\n",
    "\n",
    "@author: v-mifees\n",
    "\"\"\"\n",
    "\n",
    "#imports\n",
    "import pandas as pd\n",
    "import re\n",
    "import dash\n",
    "from dash_google_charts import WordTree\n",
    "\n",
    "text_data = pd.read_csv(\"C:\\\\Users\\\\v-mifees\\\\Desktop\\\\ocvpp_frown_filtered.csv\")['text']\n",
    "\n",
    "#functions to setup data for word tree\n",
    "def get_WordTree_data_from_Series(series,word,n):\n",
    "    \"\"\"iterates over a pandas text series and creates a \n",
    "    WordTree dataset of n words before and after each instance of word\"\"\"\n",
    "    wordtreedata = [['Phrases']]\n",
    "\n",
    "    for index, value in series.items():\n",
    "        wordtreedata.extend(get_WordTree_data(value,word,n))\n",
    "        \n",
    "    return wordtreedata\n",
    "\n",
    "def get_WordTree_data(text,word,n):\n",
    "    \"\"\"takes a text, a target word, and the number of words around it desired (n) and returns data prepared for WordTree\"\"\"\n",
    "    \n",
    "    #convert text to list of sequential words\n",
    "    text = remove_non_text(text)\n",
    "    textlist = text.split()\n",
    "    \n",
    "    #iterate over each instance of the word in the data and append its surrounding n words to the data\n",
    "    wordtreedata=[]\n",
    "    word_instances = get_word_instances(textlist,word)\n",
    "    \n",
    "    for instance in word_instances:\n",
    "        wordtreedata.append(get_words_before_after(textlist, instance, n))\n",
    "    return wordtreedata\n",
    "\n",
    "def remove_non_text(text):\n",
    "    \"\"\"removes doubled white space and non-alphabetic characters\"\"\"\n",
    "    text = re.sub(r'([^\\s\\w]|_|[1-9])+', '', text) #remove non alphanumeric/whitespace\n",
    "    text = re.sub(\" +\", \" \", text) #remove white space\n",
    "    return text\n",
    "\n",
    "def get_word_instances(textlist,word):\n",
    "    \"\"\"returns word index of each \"\"\"\n",
    "    return [i for i, w in enumerate(textlist) if w == word]\n",
    "\n",
    "def get_words_before_after(textlist,index,n):\n",
    "    \"\"\"takes a list of words and retrieves the n words before and after the word at index\"\"\"\n",
    "    \n",
    "    #initialize starting and finishing indices\n",
    "    low = index-n\n",
    "    high = index+n+1\n",
    "    \n",
    "    #filter to the last word or first word if n extends beyond the length or start of the text\n",
    "    if index - n < 0:\n",
    "        low = 0\n",
    "    elif index+n+1 >= len(textlist):\n",
    "        high = len(textlist)-1\n",
    "    \n",
    "    return [\" \".join(textlist[low:high])]\n",
    "\n",
    "#Word Tree settings\n",
    "series = text_data\n",
    "word = 'slow'\n",
    "n = 5\n",
    "\n",
    "#initializing word tree\n",
    "app = dash.Dash()\n",
    "\n",
    "app.layout = WordTree(\n",
    "    data=get_WordTree_data_from_Series(series,word,n),\n",
    "    options={\"wordtree\": {\"format\": \"implicit\", \"word\": word, 'type': 'double'}},\n",
    ")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app.run_server()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_topwords(text):\n",
    "    # Creating a tokenizer\n",
    "    tokenizer = nltk.tokenize.RegexpTokenizer('\\w+')\n",
    "\n",
    "    # Tokenizing the text\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "\n",
    "    #get all as lowercase\n",
    "    words = [w.lower() for w in tokens]\n",
    "\n",
    "    nltk.download('stopwords')\n",
    "\n",
    "    sw = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "    words_ns = []\n",
    "\n",
    "    for word in words:\n",
    "        if word not in sw:\n",
    "            words_ns.append(word)\n",
    "\n",
    "    # This command display figures inline\n",
    "    %matplotlib inline\n",
    "\n",
    "    # Creating the word frequency distribution\n",
    "    freqdist = nltk.FreqDist(words_ns)\n",
    "\n",
    "    freqdist.xlabel = 'Word'\n",
    "\n",
    "    # Plotting the word frequency distribution\n",
    "    freqdist.plot(25)\n",
    "\n",
    "graph_topwords(alltext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the wordcloud library\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# Join the different processed titles together.\n",
    "long_string = positivetext\n",
    "\n",
    "# Create a WordCloud object\n",
    "wordcloud = WordCloud()\n",
    "\n",
    "# Generate a word cloud\n",
    "wordcloud.generate(long_string)\n",
    "\n",
    "# Visualize the word cloud\n",
    "wordcloud.to_image()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collocations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#put text here\n",
    "text = 'this that then some'\n",
    "\n",
    "#tokenize text\n",
    "tokens = nltk.word_tokenize(text)\n",
    "\n",
    "#collocations\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "trigram_measures = nltk.collocations.TrigramAssocMeasures()\n",
    "finder = BigramCollocationFinder.from_words(tokens)\n",
    "numngrams = 10\n",
    "raw_collocations_bi = finder.nbest(bigram_measures.pmi, numngrams)\n",
    "\n",
    "#applying frequency filter\n",
    "min_occurences = 3\n",
    "finder.apply_freq_filter(min_occurences)\n",
    "filt_collocations_bi = finder.nbest(bigram_measures.pmi, numngrams)\n",
    "\n",
    "#trigrams\n",
    "finder = TrigramCollocationFinder.from_words(tokens)\n",
    "scored = finder.score_ngrams(trigram_measures.raw_freq)\n",
    "\n",
    "#top n results\n",
    "sorted(finder.nbest(trigram_measures.raw_freq, 2))\n",
    "\n",
    "#spanning intervening words\n",
    "finder = TrigramCollocationFinder.from_words(tokens, window_size=4)\n",
    "sorted(finder.nbest(trigram_measures.raw_freq, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Appendix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code in waiting\n",
    "#test cell\n",
    "import re\n",
    "[m.start() for m in re.finditer('test', 'test test test test')]\n",
    "\n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "def similar(a, b):\n",
    "    return SequenceMatcher(None, a, b).ratio()\n",
    "\n",
    "similar(\"love\",\"loves\")"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": false,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
